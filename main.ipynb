{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-04T11:26:15.656401Z",
     "start_time": "2025-07-04T11:26:15.653982Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:26:15.695907Z",
     "start_time": "2025-07-04T11:26:15.668250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set input/output dimensions\n",
    "m = 7  # input size\n",
    "p = 4  # output size\n",
    "h = 10  # hidden dimension (number of REN units)\n",
    "\n",
    "# Load training data\n",
    "dataset_csv = pd.read_csv('dataset_xid_train_shuffle.csv', sep=',')\n",
    "dataset = np.array(dataset_csv.iloc[:, 0:p + m])\n",
    "X_train = torch.tensor(dataset[:, p:], dtype=torch.float32)\n",
    "Y_train = torch.tensor(dataset[:, :p], dtype=torch.float32)\n",
    "\n",
    "# Load validation data\n",
    "dataset_csv_val = pd.read_csv('dataset_xid_val_shuffle.csv', sep=',')\n",
    "dataset_val = np.array(dataset_csv_val.iloc[:, 0:p + m])\n",
    "X_val = torch.tensor(dataset_val[:, p:], dtype=torch.float32)\n",
    "Y_val = torch.tensor(dataset_val[:, :p], dtype=torch.float32)"
   ],
   "id": "a43d8c8e4ef8b15a",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:26:15.723186Z",
     "start_time": "2025-07-04T11:26:15.714389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === REN Model ===\n",
    "class ExplicitREN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.B = nn.Linear(input_dim, hidden_dim, bias=False)      # B: input-to-state\n",
    "        self.Bs_full = nn.Parameter(torch.empty(hidden_dim, hidden_dim))  # Bs: strictly lower triangular recurrence\n",
    "        self.Ds = nn.Linear(hidden_dim, output_dim, bias=False)    # Ds: state-to-output\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=False)      # D: direct input-to-output\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        # Initialize and apply strict lower triangular mask to Bs\n",
    "        nn.init.xavier_uniform_(self.Bs_full)\n",
    "        tril_mask = torch.tril(torch.ones_like(self.Bs_full), diagonal=-1)\n",
    "        self.register_buffer(\"tril_mask\", tril_mask)\n",
    "\n",
    "    def forward(self, u):\n",
    "        \"\"\"\n",
    "        Implements an explicit REN with:\n",
    "            s_i = tanh( âˆ‘_{j < i} Bs_{ij} s_j + (B u)_i )\n",
    "            y = Ds s + D u\n",
    "\n",
    "        where Bs is strictly lower triangular.\n",
    "        \"\"\"\n",
    "        batch_size = u.shape[0]\n",
    "        hidden_dim = self.Bs_full.shape[0]\n",
    "\n",
    "        Bs = self.Bs_full * self.tril_mask\n",
    "        Bu = self.B(u)  # shape: (batch_size, hidden_dim)\n",
    "\n",
    "        s_list = []\n",
    "        for i in range(hidden_dim):\n",
    "            if i == 0:\n",
    "                s_i = self.activation(Bu[:, i])\n",
    "            else:\n",
    "                s_prev = torch.stack(s_list, dim=1)\n",
    "                bs_row = Bs[i, :i]\n",
    "                bsz = torch.matmul(s_prev, bs_row.T)\n",
    "                s_i = self.activation(bsz + Bu[:, i])\n",
    "            s_list.append(s_i)\n",
    "\n",
    "        s = torch.stack(s_list, dim=1)  # shape: (batch_size, hidden_dim)\n",
    "        y = self.Ds(s) + self.D(u)      # final output\n",
    "        return y"
   ],
   "id": "7f959fbd3799407f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:26:15.759880Z",
     "start_time": "2025-07-04T11:26:15.753240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate model and training components\n",
    "model = ExplicitREN(m, h, p)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=64)\n"
   ],
   "id": "9e84663f4316933f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-04T11:26:15.785840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(2000):\n",
    "    if epoch == 1500:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(x), y).item() for x, y in val_loader) / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}\")\n"
   ],
   "id": "8a6e7f0ba61eca1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.170364, Val Loss: 0.094894\n",
      "Epoch 2, Train Loss: 0.080881, Val Loss: 0.068091\n",
      "Epoch 3, Train Loss: 0.066078, Val Loss: 0.061543\n",
      "Epoch 4, Train Loss: 0.061371, Val Loss: 0.058647\n",
      "Epoch 5, Train Loss: 0.058929, Val Loss: 0.056709\n",
      "Epoch 6, Train Loss: 0.057004, Val Loss: 0.054652\n",
      "Epoch 7, Train Loss: 0.055010, Val Loss: 0.052851\n",
      "Epoch 8, Train Loss: 0.053027, Val Loss: 0.050621\n",
      "Epoch 9, Train Loss: 0.050942, Val Loss: 0.048795\n",
      "Epoch 10, Train Loss: 0.049145, Val Loss: 0.047609\n",
      "Epoch 11, Train Loss: 0.047626, Val Loss: 0.045824\n",
      "Epoch 12, Train Loss: 0.046246, Val Loss: 0.044615\n",
      "Epoch 13, Train Loss: 0.045002, Val Loss: 0.043624\n",
      "Epoch 14, Train Loss: 0.043871, Val Loss: 0.042713\n",
      "Epoch 15, Train Loss: 0.042945, Val Loss: 0.041777\n",
      "Epoch 16, Train Loss: 0.042008, Val Loss: 0.041027\n",
      "Epoch 17, Train Loss: 0.041256, Val Loss: 0.040386\n",
      "Epoch 18, Train Loss: 0.040476, Val Loss: 0.039864\n",
      "Epoch 19, Train Loss: 0.039837, Val Loss: 0.039195\n",
      "Epoch 20, Train Loss: 0.039286, Val Loss: 0.038780\n",
      "Epoch 21, Train Loss: 0.038740, Val Loss: 0.038375\n",
      "Epoch 22, Train Loss: 0.038282, Val Loss: 0.037847\n",
      "Epoch 23, Train Loss: 0.037799, Val Loss: 0.037382\n",
      "Epoch 24, Train Loss: 0.037333, Val Loss: 0.036971\n",
      "Epoch 25, Train Loss: 0.036899, Val Loss: 0.036606\n",
      "Epoch 26, Train Loss: 0.036467, Val Loss: 0.036172\n",
      "Epoch 27, Train Loss: 0.036094, Val Loss: 0.035824\n",
      "Epoch 28, Train Loss: 0.035656, Val Loss: 0.035375\n",
      "Epoch 29, Train Loss: 0.035338, Val Loss: 0.035038\n",
      "Epoch 30, Train Loss: 0.034877, Val Loss: 0.034868\n",
      "Epoch 31, Train Loss: 0.034557, Val Loss: 0.034340\n",
      "Epoch 32, Train Loss: 0.034233, Val Loss: 0.034279\n",
      "Epoch 33, Train Loss: 0.033895, Val Loss: 0.033743\n",
      "Epoch 34, Train Loss: 0.033526, Val Loss: 0.033594\n",
      "Epoch 35, Train Loss: 0.033255, Val Loss: 0.033297\n",
      "Epoch 36, Train Loss: 0.033017, Val Loss: 0.032953\n",
      "Epoch 37, Train Loss: 0.032752, Val Loss: 0.032797\n",
      "Epoch 38, Train Loss: 0.032527, Val Loss: 0.032564\n",
      "Epoch 39, Train Loss: 0.032278, Val Loss: 0.032744\n",
      "Epoch 40, Train Loss: 0.032079, Val Loss: 0.032222\n",
      "Epoch 41, Train Loss: 0.031889, Val Loss: 0.032099\n",
      "Epoch 42, Train Loss: 0.031815, Val Loss: 0.031924\n",
      "Epoch 43, Train Loss: 0.031708, Val Loss: 0.031849\n",
      "Epoch 44, Train Loss: 0.031513, Val Loss: 0.031974\n",
      "Epoch 45, Train Loss: 0.031441, Val Loss: 0.031656\n",
      "Epoch 46, Train Loss: 0.031333, Val Loss: 0.032096\n",
      "Epoch 47, Train Loss: 0.031236, Val Loss: 0.031526\n",
      "Epoch 48, Train Loss: 0.031160, Val Loss: 0.031491\n",
      "Epoch 49, Train Loss: 0.031107, Val Loss: 0.031451\n",
      "Epoch 50, Train Loss: 0.031015, Val Loss: 0.031319\n",
      "Epoch 51, Train Loss: 0.030953, Val Loss: 0.031254\n",
      "Epoch 52, Train Loss: 0.030901, Val Loss: 0.031172\n",
      "Epoch 53, Train Loss: 0.030889, Val Loss: 0.031214\n",
      "Epoch 54, Train Loss: 0.030826, Val Loss: 0.031195\n",
      "Epoch 55, Train Loss: 0.030762, Val Loss: 0.031245\n",
      "Epoch 56, Train Loss: 0.030682, Val Loss: 0.031504\n",
      "Epoch 57, Train Loss: 0.030724, Val Loss: 0.031070\n",
      "Epoch 58, Train Loss: 0.030626, Val Loss: 0.030933\n",
      "Epoch 59, Train Loss: 0.030587, Val Loss: 0.030905\n",
      "Epoch 60, Train Loss: 0.030527, Val Loss: 0.030826\n",
      "Epoch 61, Train Loss: 0.030461, Val Loss: 0.030848\n",
      "Epoch 62, Train Loss: 0.030422, Val Loss: 0.031029\n",
      "Epoch 63, Train Loss: 0.030358, Val Loss: 0.030741\n",
      "Epoch 64, Train Loss: 0.030323, Val Loss: 0.030748\n",
      "Epoch 65, Train Loss: 0.030284, Val Loss: 0.030649\n",
      "Epoch 66, Train Loss: 0.030312, Val Loss: 0.030666\n",
      "Epoch 67, Train Loss: 0.030244, Val Loss: 0.030975\n",
      "Epoch 68, Train Loss: 0.030217, Val Loss: 0.030720\n",
      "Epoch 69, Train Loss: 0.030240, Val Loss: 0.030591\n",
      "Epoch 70, Train Loss: 0.030108, Val Loss: 0.030582\n",
      "Epoch 71, Train Loss: 0.030122, Val Loss: 0.030614\n",
      "Epoch 72, Train Loss: 0.030038, Val Loss: 0.030584\n",
      "Epoch 73, Train Loss: 0.030049, Val Loss: 0.030579\n",
      "Epoch 74, Train Loss: 0.029987, Val Loss: 0.030418\n",
      "Epoch 75, Train Loss: 0.030026, Val Loss: 0.030546\n",
      "Epoch 76, Train Loss: 0.029935, Val Loss: 0.030373\n",
      "Epoch 77, Train Loss: 0.029880, Val Loss: 0.030312\n",
      "Epoch 78, Train Loss: 0.029873, Val Loss: 0.030625\n",
      "Epoch 79, Train Loss: 0.029958, Val Loss: 0.030242\n",
      "Epoch 80, Train Loss: 0.029861, Val Loss: 0.030253\n",
      "Epoch 81, Train Loss: 0.029743, Val Loss: 0.030430\n",
      "Epoch 82, Train Loss: 0.029829, Val Loss: 0.030354\n",
      "Epoch 83, Train Loss: 0.029815, Val Loss: 0.030519\n",
      "Epoch 84, Train Loss: 0.029697, Val Loss: 0.030285\n",
      "Epoch 85, Train Loss: 0.029706, Val Loss: 0.030227\n",
      "Epoch 86, Train Loss: 0.029722, Val Loss: 0.030261\n",
      "Epoch 87, Train Loss: 0.029649, Val Loss: 0.030125\n",
      "Epoch 88, Train Loss: 0.029637, Val Loss: 0.030102\n",
      "Epoch 89, Train Loss: 0.029709, Val Loss: 0.030147\n",
      "Epoch 90, Train Loss: 0.029561, Val Loss: 0.030129\n",
      "Epoch 91, Train Loss: 0.029611, Val Loss: 0.030030\n",
      "Epoch 92, Train Loss: 0.029536, Val Loss: 0.030039\n",
      "Epoch 93, Train Loss: 0.029471, Val Loss: 0.030093\n",
      "Epoch 94, Train Loss: 0.029480, Val Loss: 0.030080\n",
      "Epoch 95, Train Loss: 0.029455, Val Loss: 0.030019\n",
      "Epoch 96, Train Loss: 0.029424, Val Loss: 0.029969\n",
      "Epoch 97, Train Loss: 0.029429, Val Loss: 0.030071\n",
      "Epoch 98, Train Loss: 0.029377, Val Loss: 0.029933\n",
      "Epoch 99, Train Loss: 0.029427, Val Loss: 0.030022\n",
      "Epoch 100, Train Loss: 0.029440, Val Loss: 0.029890\n",
      "Epoch 101, Train Loss: 0.029340, Val Loss: 0.029897\n",
      "Epoch 102, Train Loss: 0.029310, Val Loss: 0.029839\n",
      "Epoch 103, Train Loss: 0.029292, Val Loss: 0.029914\n",
      "Epoch 104, Train Loss: 0.029288, Val Loss: 0.029952\n",
      "Epoch 105, Train Loss: 0.029260, Val Loss: 0.029810\n",
      "Epoch 106, Train Loss: 0.029188, Val Loss: 0.029802\n",
      "Epoch 107, Train Loss: 0.029236, Val Loss: 0.029773\n",
      "Epoch 108, Train Loss: 0.029210, Val Loss: 0.029840\n",
      "Epoch 109, Train Loss: 0.029203, Val Loss: 0.029831\n",
      "Epoch 110, Train Loss: 0.029202, Val Loss: 0.029649\n",
      "Epoch 111, Train Loss: 0.029133, Val Loss: 0.029618\n",
      "Epoch 112, Train Loss: 0.029094, Val Loss: 0.029667\n",
      "Epoch 113, Train Loss: 0.029095, Val Loss: 0.029723\n",
      "Epoch 114, Train Loss: 0.029098, Val Loss: 0.029632\n",
      "Epoch 115, Train Loss: 0.029027, Val Loss: 0.029676\n",
      "Epoch 116, Train Loss: 0.028997, Val Loss: 0.029640\n",
      "Epoch 117, Train Loss: 0.029010, Val Loss: 0.029570\n",
      "Epoch 118, Train Loss: 0.028982, Val Loss: 0.029618\n",
      "Epoch 119, Train Loss: 0.029004, Val Loss: 0.029739\n",
      "Epoch 120, Train Loss: 0.028984, Val Loss: 0.029555\n",
      "Epoch 121, Train Loss: 0.028926, Val Loss: 0.029530\n",
      "Epoch 122, Train Loss: 0.028939, Val Loss: 0.029520\n",
      "Epoch 123, Train Loss: 0.028888, Val Loss: 0.029414\n",
      "Epoch 124, Train Loss: 0.028898, Val Loss: 0.029455\n",
      "Epoch 125, Train Loss: 0.028933, Val Loss: 0.029490\n",
      "Epoch 126, Train Loss: 0.028838, Val Loss: 0.029546\n",
      "Epoch 127, Train Loss: 0.028887, Val Loss: 0.029425\n",
      "Epoch 128, Train Loss: 0.028820, Val Loss: 0.029557\n",
      "Epoch 129, Train Loss: 0.028848, Val Loss: 0.029356\n",
      "Epoch 130, Train Loss: 0.028804, Val Loss: 0.029536\n",
      "Epoch 131, Train Loss: 0.028809, Val Loss: 0.029450\n",
      "Epoch 132, Train Loss: 0.028861, Val Loss: 0.029437\n",
      "Epoch 133, Train Loss: 0.028777, Val Loss: 0.029345\n",
      "Epoch 134, Train Loss: 0.028763, Val Loss: 0.029586\n",
      "Epoch 135, Train Loss: 0.028765, Val Loss: 0.029851\n",
      "Epoch 136, Train Loss: 0.028820, Val Loss: 0.029301\n",
      "Epoch 137, Train Loss: 0.028698, Val Loss: 0.029344\n",
      "Epoch 138, Train Loss: 0.028781, Val Loss: 0.029358\n",
      "Epoch 139, Train Loss: 0.028733, Val Loss: 0.029401\n",
      "Epoch 140, Train Loss: 0.028705, Val Loss: 0.029270\n",
      "Epoch 141, Train Loss: 0.028693, Val Loss: 0.029341\n",
      "Epoch 142, Train Loss: 0.028664, Val Loss: 0.029192\n",
      "Epoch 143, Train Loss: 0.028724, Val Loss: 0.029233\n",
      "Epoch 144, Train Loss: 0.028672, Val Loss: 0.029232\n",
      "Epoch 145, Train Loss: 0.028721, Val Loss: 0.029240\n",
      "Epoch 146, Train Loss: 0.028600, Val Loss: 0.029169\n",
      "Epoch 147, Train Loss: 0.028626, Val Loss: 0.029201\n",
      "Epoch 148, Train Loss: 0.028592, Val Loss: 0.029245\n",
      "Epoch 149, Train Loss: 0.028592, Val Loss: 0.029310\n",
      "Epoch 150, Train Loss: 0.028573, Val Loss: 0.029163\n",
      "Epoch 151, Train Loss: 0.028551, Val Loss: 0.029273\n",
      "Epoch 152, Train Loss: 0.028561, Val Loss: 0.029196\n",
      "Epoch 153, Train Loss: 0.028512, Val Loss: 0.029195\n",
      "Epoch 154, Train Loss: 0.028558, Val Loss: 0.029176\n",
      "Epoch 155, Train Loss: 0.028536, Val Loss: 0.029112\n",
      "Epoch 156, Train Loss: 0.028537, Val Loss: 0.029156\n",
      "Epoch 157, Train Loss: 0.028483, Val Loss: 0.029077\n",
      "Epoch 158, Train Loss: 0.028522, Val Loss: 0.029098\n",
      "Epoch 159, Train Loss: 0.028510, Val Loss: 0.029093\n",
      "Epoch 160, Train Loss: 0.028461, Val Loss: 0.029097\n",
      "Epoch 161, Train Loss: 0.028463, Val Loss: 0.029144\n",
      "Epoch 162, Train Loss: 0.028485, Val Loss: 0.029280\n",
      "Epoch 163, Train Loss: 0.028462, Val Loss: 0.029022\n",
      "Epoch 164, Train Loss: 0.028492, Val Loss: 0.029113\n",
      "Epoch 165, Train Loss: 0.028497, Val Loss: 0.029076\n",
      "Epoch 166, Train Loss: 0.028408, Val Loss: 0.029203\n",
      "Epoch 167, Train Loss: 0.028424, Val Loss: 0.029049\n",
      "Epoch 168, Train Loss: 0.028446, Val Loss: 0.028996\n",
      "Epoch 169, Train Loss: 0.028438, Val Loss: 0.029058\n",
      "Epoch 170, Train Loss: 0.028395, Val Loss: 0.029031\n",
      "Epoch 171, Train Loss: 0.028482, Val Loss: 0.028985\n",
      "Epoch 172, Train Loss: 0.028486, Val Loss: 0.028962\n",
      "Epoch 173, Train Loss: 0.028354, Val Loss: 0.028982\n",
      "Epoch 174, Train Loss: 0.028373, Val Loss: 0.028969\n",
      "Epoch 175, Train Loss: 0.028380, Val Loss: 0.028912\n",
      "Epoch 176, Train Loss: 0.028353, Val Loss: 0.028950\n",
      "Epoch 177, Train Loss: 0.028401, Val Loss: 0.028914\n",
      "Epoch 178, Train Loss: 0.028321, Val Loss: 0.028967\n",
      "Epoch 179, Train Loss: 0.028352, Val Loss: 0.029101\n",
      "Epoch 180, Train Loss: 0.028362, Val Loss: 0.028885\n",
      "Epoch 181, Train Loss: 0.028334, Val Loss: 0.028862\n",
      "Epoch 182, Train Loss: 0.028365, Val Loss: 0.028925\n",
      "Epoch 183, Train Loss: 0.028294, Val Loss: 0.028809\n",
      "Epoch 184, Train Loss: 0.028349, Val Loss: 0.028875\n",
      "Epoch 185, Train Loss: 0.028295, Val Loss: 0.028914\n",
      "Epoch 186, Train Loss: 0.028294, Val Loss: 0.028951\n",
      "Epoch 187, Train Loss: 0.028296, Val Loss: 0.028856\n",
      "Epoch 188, Train Loss: 0.028302, Val Loss: 0.028811\n",
      "Epoch 189, Train Loss: 0.028225, Val Loss: 0.028763\n",
      "Epoch 190, Train Loss: 0.028245, Val Loss: 0.028947\n",
      "Epoch 191, Train Loss: 0.028291, Val Loss: 0.028862\n",
      "Epoch 192, Train Loss: 0.028228, Val Loss: 0.028943\n",
      "Epoch 193, Train Loss: 0.028288, Val Loss: 0.028769\n",
      "Epoch 194, Train Loss: 0.028180, Val Loss: 0.028826\n",
      "Epoch 195, Train Loss: 0.028200, Val Loss: 0.028724\n",
      "Epoch 196, Train Loss: 0.028201, Val Loss: 0.028713\n",
      "Epoch 197, Train Loss: 0.028212, Val Loss: 0.028779\n",
      "Epoch 198, Train Loss: 0.028210, Val Loss: 0.028714\n",
      "Epoch 199, Train Loss: 0.028236, Val Loss: 0.028706\n",
      "Epoch 200, Train Loss: 0.028115, Val Loss: 0.028789\n",
      "Epoch 201, Train Loss: 0.028174, Val Loss: 0.028867\n",
      "Epoch 202, Train Loss: 0.028159, Val Loss: 0.028681\n",
      "Epoch 203, Train Loss: 0.028143, Val Loss: 0.028788\n",
      "Epoch 204, Train Loss: 0.028148, Val Loss: 0.028790\n",
      "Epoch 205, Train Loss: 0.028155, Val Loss: 0.028631\n",
      "Epoch 206, Train Loss: 0.028148, Val Loss: 0.028688\n",
      "Epoch 207, Train Loss: 0.028145, Val Loss: 0.028690\n",
      "Epoch 208, Train Loss: 0.028140, Val Loss: 0.028661\n",
      "Epoch 209, Train Loss: 0.028166, Val Loss: 0.028679\n",
      "Epoch 210, Train Loss: 0.028126, Val Loss: 0.028838\n",
      "Epoch 211, Train Loss: 0.028099, Val Loss: 0.028581\n",
      "Epoch 212, Train Loss: 0.028068, Val Loss: 0.028629\n",
      "Epoch 213, Train Loss: 0.028082, Val Loss: 0.028649\n",
      "Epoch 214, Train Loss: 0.028059, Val Loss: 0.028622\n",
      "Epoch 215, Train Loss: 0.028067, Val Loss: 0.028662\n",
      "Epoch 216, Train Loss: 0.028115, Val Loss: 0.028848\n",
      "Epoch 217, Train Loss: 0.028110, Val Loss: 0.028587\n",
      "Epoch 218, Train Loss: 0.028063, Val Loss: 0.028676\n",
      "Epoch 219, Train Loss: 0.028058, Val Loss: 0.028574\n",
      "Epoch 220, Train Loss: 0.028002, Val Loss: 0.028964\n",
      "Epoch 221, Train Loss: 0.028055, Val Loss: 0.028775\n",
      "Epoch 222, Train Loss: 0.028038, Val Loss: 0.028569\n",
      "Epoch 223, Train Loss: 0.028009, Val Loss: 0.028580\n",
      "Epoch 224, Train Loss: 0.028006, Val Loss: 0.028750\n",
      "Epoch 225, Train Loss: 0.027984, Val Loss: 0.028506\n",
      "Epoch 226, Train Loss: 0.027967, Val Loss: 0.028567\n",
      "Epoch 227, Train Loss: 0.027964, Val Loss: 0.028539\n",
      "Epoch 228, Train Loss: 0.028018, Val Loss: 0.028517\n",
      "Epoch 229, Train Loss: 0.028073, Val Loss: 0.028496\n",
      "Epoch 230, Train Loss: 0.027973, Val Loss: 0.028687\n",
      "Epoch 231, Train Loss: 0.028034, Val Loss: 0.028467\n",
      "Epoch 232, Train Loss: 0.027919, Val Loss: 0.028488\n",
      "Epoch 233, Train Loss: 0.027965, Val Loss: 0.028507\n",
      "Epoch 234, Train Loss: 0.027928, Val Loss: 0.028531\n",
      "Epoch 235, Train Loss: 0.027964, Val Loss: 0.028546\n",
      "Epoch 236, Train Loss: 0.027919, Val Loss: 0.028514\n",
      "Epoch 237, Train Loss: 0.028018, Val Loss: 0.028411\n",
      "Epoch 238, Train Loss: 0.027885, Val Loss: 0.028420\n",
      "Epoch 239, Train Loss: 0.027856, Val Loss: 0.028407\n",
      "Epoch 240, Train Loss: 0.027947, Val Loss: 0.028428\n",
      "Epoch 241, Train Loss: 0.027890, Val Loss: 0.028484\n",
      "Epoch 242, Train Loss: 0.027850, Val Loss: 0.028614\n",
      "Epoch 243, Train Loss: 0.027842, Val Loss: 0.028395\n",
      "Epoch 244, Train Loss: 0.027896, Val Loss: 0.028350\n",
      "Epoch 245, Train Loss: 0.027900, Val Loss: 0.028395\n",
      "Epoch 246, Train Loss: 0.027838, Val Loss: 0.028455\n",
      "Epoch 247, Train Loss: 0.027855, Val Loss: 0.028348\n",
      "Epoch 248, Train Loss: 0.027853, Val Loss: 0.028614\n",
      "Epoch 249, Train Loss: 0.027915, Val Loss: 0.028345\n",
      "Epoch 250, Train Loss: 0.027858, Val Loss: 0.028356\n",
      "Epoch 251, Train Loss: 0.027792, Val Loss: 0.028378\n",
      "Epoch 252, Train Loss: 0.027770, Val Loss: 0.028311\n",
      "Epoch 253, Train Loss: 0.027778, Val Loss: 0.028542\n",
      "Epoch 254, Train Loss: 0.027773, Val Loss: 0.028264\n",
      "Epoch 255, Train Loss: 0.027776, Val Loss: 0.028319\n",
      "Epoch 256, Train Loss: 0.027816, Val Loss: 0.028314\n",
      "Epoch 257, Train Loss: 0.027740, Val Loss: 0.028334\n",
      "Epoch 258, Train Loss: 0.027795, Val Loss: 0.028479\n",
      "Epoch 259, Train Loss: 0.027771, Val Loss: 0.028295\n",
      "Epoch 260, Train Loss: 0.027761, Val Loss: 0.028278\n",
      "Epoch 261, Train Loss: 0.027703, Val Loss: 0.028310\n",
      "Epoch 262, Train Loss: 0.027718, Val Loss: 0.028363\n",
      "Epoch 263, Train Loss: 0.027773, Val Loss: 0.028343\n",
      "Epoch 264, Train Loss: 0.027701, Val Loss: 0.028328\n",
      "Epoch 265, Train Loss: 0.027678, Val Loss: 0.028545\n",
      "Epoch 266, Train Loss: 0.027763, Val Loss: 0.028307\n",
      "Epoch 267, Train Loss: 0.027658, Val Loss: 0.028253\n",
      "Epoch 268, Train Loss: 0.027673, Val Loss: 0.028298\n",
      "Epoch 269, Train Loss: 0.027673, Val Loss: 0.028267\n",
      "Epoch 270, Train Loss: 0.027684, Val Loss: 0.028359\n",
      "Epoch 271, Train Loss: 0.027630, Val Loss: 0.028146\n",
      "Epoch 272, Train Loss: 0.027664, Val Loss: 0.028130\n",
      "Epoch 273, Train Loss: 0.027690, Val Loss: 0.028259\n",
      "Epoch 274, Train Loss: 0.027625, Val Loss: 0.028204\n",
      "Epoch 275, Train Loss: 0.027651, Val Loss: 0.028191\n",
      "Epoch 276, Train Loss: 0.027575, Val Loss: 0.028066\n",
      "Epoch 277, Train Loss: 0.027618, Val Loss: 0.028146\n",
      "Epoch 278, Train Loss: 0.027553, Val Loss: 0.028110\n",
      "Epoch 279, Train Loss: 0.027530, Val Loss: 0.028454\n",
      "Epoch 280, Train Loss: 0.027540, Val Loss: 0.028118\n",
      "Epoch 281, Train Loss: 0.027611, Val Loss: 0.028143\n",
      "Epoch 282, Train Loss: 0.027614, Val Loss: 0.028457\n",
      "Epoch 283, Train Loss: 0.027550, Val Loss: 0.028195\n",
      "Epoch 284, Train Loss: 0.027506, Val Loss: 0.028053\n",
      "Epoch 285, Train Loss: 0.027484, Val Loss: 0.027953\n",
      "Epoch 286, Train Loss: 0.027489, Val Loss: 0.028110\n",
      "Epoch 287, Train Loss: 0.027446, Val Loss: 0.028193\n",
      "Epoch 288, Train Loss: 0.027468, Val Loss: 0.027946\n",
      "Epoch 289, Train Loss: 0.027395, Val Loss: 0.027936\n",
      "Epoch 290, Train Loss: 0.027457, Val Loss: 0.028164\n",
      "Epoch 291, Train Loss: 0.027396, Val Loss: 0.027913\n",
      "Epoch 292, Train Loss: 0.027419, Val Loss: 0.028016\n",
      "Epoch 293, Train Loss: 0.027397, Val Loss: 0.027883\n",
      "Epoch 294, Train Loss: 0.027451, Val Loss: 0.027998\n",
      "Epoch 295, Train Loss: 0.027360, Val Loss: 0.028050\n",
      "Epoch 296, Train Loss: 0.027419, Val Loss: 0.027909\n",
      "Epoch 297, Train Loss: 0.027366, Val Loss: 0.028027\n",
      "Epoch 298, Train Loss: 0.027420, Val Loss: 0.027823\n",
      "Epoch 299, Train Loss: 0.027318, Val Loss: 0.027848\n",
      "Epoch 300, Train Loss: 0.027267, Val Loss: 0.027900\n",
      "Epoch 301, Train Loss: 0.027368, Val Loss: 0.027732\n",
      "Epoch 302, Train Loss: 0.027317, Val Loss: 0.027840\n",
      "Epoch 303, Train Loss: 0.027294, Val Loss: 0.027749\n",
      "Epoch 304, Train Loss: 0.027267, Val Loss: 0.027656\n",
      "Epoch 305, Train Loss: 0.027250, Val Loss: 0.027665\n",
      "Epoch 306, Train Loss: 0.027223, Val Loss: 0.027804\n",
      "Epoch 307, Train Loss: 0.027249, Val Loss: 0.027703\n",
      "Epoch 308, Train Loss: 0.027232, Val Loss: 0.027575\n",
      "Epoch 309, Train Loss: 0.027219, Val Loss: 0.027919\n",
      "Epoch 310, Train Loss: 0.027314, Val Loss: 0.027641\n",
      "Epoch 311, Train Loss: 0.027207, Val Loss: 0.027762\n",
      "Epoch 312, Train Loss: 0.027176, Val Loss: 0.027533\n",
      "Epoch 313, Train Loss: 0.027135, Val Loss: 0.027807\n",
      "Epoch 314, Train Loss: 0.027184, Val Loss: 0.027582\n",
      "Epoch 315, Train Loss: 0.027126, Val Loss: 0.027646\n",
      "Epoch 316, Train Loss: 0.027172, Val Loss: 0.027661\n",
      "Epoch 317, Train Loss: 0.027137, Val Loss: 0.027863\n",
      "Epoch 318, Train Loss: 0.027140, Val Loss: 0.027594\n",
      "Epoch 319, Train Loss: 0.027087, Val Loss: 0.027580\n",
      "Epoch 320, Train Loss: 0.027130, Val Loss: 0.027634\n",
      "Epoch 321, Train Loss: 0.027217, Val Loss: 0.027782\n",
      "Epoch 322, Train Loss: 0.027096, Val Loss: 0.027553\n",
      "Epoch 323, Train Loss: 0.026991, Val Loss: 0.027529\n",
      "Epoch 324, Train Loss: 0.026996, Val Loss: 0.027444\n",
      "Epoch 325, Train Loss: 0.026995, Val Loss: 0.027657\n",
      "Epoch 326, Train Loss: 0.027044, Val Loss: 0.027369\n",
      "Epoch 327, Train Loss: 0.027058, Val Loss: 0.027448\n",
      "Epoch 328, Train Loss: 0.026961, Val Loss: 0.027378\n",
      "Epoch 329, Train Loss: 0.027027, Val Loss: 0.027411\n",
      "Epoch 330, Train Loss: 0.026988, Val Loss: 0.027468\n",
      "Epoch 331, Train Loss: 0.026961, Val Loss: 0.027337\n",
      "Epoch 332, Train Loss: 0.027115, Val Loss: 0.027435\n",
      "Epoch 333, Train Loss: 0.026915, Val Loss: 0.027347\n",
      "Epoch 334, Train Loss: 0.026951, Val Loss: 0.027509\n",
      "Epoch 335, Train Loss: 0.026918, Val Loss: 0.027427\n",
      "Epoch 336, Train Loss: 0.026948, Val Loss: 0.027250\n",
      "Epoch 337, Train Loss: 0.026939, Val Loss: 0.027234\n",
      "Epoch 338, Train Loss: 0.026955, Val Loss: 0.027449\n",
      "Epoch 339, Train Loss: 0.026930, Val Loss: 0.027571\n",
      "Epoch 340, Train Loss: 0.026949, Val Loss: 0.027276\n",
      "Epoch 341, Train Loss: 0.026915, Val Loss: 0.027320\n",
      "Epoch 342, Train Loss: 0.026860, Val Loss: 0.027198\n",
      "Epoch 343, Train Loss: 0.026813, Val Loss: 0.027301\n",
      "Epoch 344, Train Loss: 0.026865, Val Loss: 0.027207\n",
      "Epoch 345, Train Loss: 0.026839, Val Loss: 0.027442\n",
      "Epoch 346, Train Loss: 0.026826, Val Loss: 0.027109\n",
      "Epoch 347, Train Loss: 0.026789, Val Loss: 0.027427\n",
      "Epoch 348, Train Loss: 0.026822, Val Loss: 0.027341\n",
      "Epoch 349, Train Loss: 0.026834, Val Loss: 0.027240\n",
      "Epoch 350, Train Loss: 0.026752, Val Loss: 0.027166\n",
      "Epoch 351, Train Loss: 0.026811, Val Loss: 0.027232\n",
      "Epoch 352, Train Loss: 0.026773, Val Loss: 0.027093\n",
      "Epoch 353, Train Loss: 0.026781, Val Loss: 0.027129\n",
      "Epoch 354, Train Loss: 0.026788, Val Loss: 0.027012\n",
      "Epoch 355, Train Loss: 0.026781, Val Loss: 0.027115\n",
      "Epoch 356, Train Loss: 0.026743, Val Loss: 0.027205\n",
      "Epoch 357, Train Loss: 0.026785, Val Loss: 0.027378\n",
      "Epoch 358, Train Loss: 0.026776, Val Loss: 0.027297\n",
      "Epoch 359, Train Loss: 0.026699, Val Loss: 0.027149\n",
      "Epoch 360, Train Loss: 0.026741, Val Loss: 0.027279\n",
      "Epoch 361, Train Loss: 0.026752, Val Loss: 0.027266\n",
      "Epoch 362, Train Loss: 0.026714, Val Loss: 0.026964\n",
      "Epoch 363, Train Loss: 0.026700, Val Loss: 0.027096\n",
      "Epoch 364, Train Loss: 0.026725, Val Loss: 0.027245\n",
      "Epoch 365, Train Loss: 0.026751, Val Loss: 0.027067\n",
      "Epoch 366, Train Loss: 0.026738, Val Loss: 0.027090\n",
      "Epoch 367, Train Loss: 0.026694, Val Loss: 0.027004\n",
      "Epoch 368, Train Loss: 0.026626, Val Loss: 0.027152\n",
      "Epoch 369, Train Loss: 0.026653, Val Loss: 0.027057\n",
      "Epoch 370, Train Loss: 0.026606, Val Loss: 0.026963\n",
      "Epoch 371, Train Loss: 0.026681, Val Loss: 0.027110\n",
      "Epoch 372, Train Loss: 0.026634, Val Loss: 0.026988\n",
      "Epoch 373, Train Loss: 0.026624, Val Loss: 0.027176\n",
      "Epoch 374, Train Loss: 0.026585, Val Loss: 0.026927\n",
      "Epoch 375, Train Loss: 0.026603, Val Loss: 0.027166\n",
      "Epoch 376, Train Loss: 0.026653, Val Loss: 0.026906\n",
      "Epoch 377, Train Loss: 0.026615, Val Loss: 0.026912\n",
      "Epoch 378, Train Loss: 0.026645, Val Loss: 0.026925\n",
      "Epoch 379, Train Loss: 0.026632, Val Loss: 0.027091\n",
      "Epoch 380, Train Loss: 0.026607, Val Loss: 0.026994\n",
      "Epoch 381, Train Loss: 0.026598, Val Loss: 0.026934\n",
      "Epoch 382, Train Loss: 0.026576, Val Loss: 0.027125\n",
      "Epoch 383, Train Loss: 0.026606, Val Loss: 0.026957\n",
      "Epoch 384, Train Loss: 0.026569, Val Loss: 0.027037\n",
      "Epoch 385, Train Loss: 0.026574, Val Loss: 0.026876\n",
      "Epoch 386, Train Loss: 0.026536, Val Loss: 0.026974\n",
      "Epoch 387, Train Loss: 0.026558, Val Loss: 0.027259\n",
      "Epoch 388, Train Loss: 0.026511, Val Loss: 0.027046\n",
      "Epoch 389, Train Loss: 0.026551, Val Loss: 0.026993\n",
      "Epoch 390, Train Loss: 0.026552, Val Loss: 0.026860\n",
      "Epoch 391, Train Loss: 0.026489, Val Loss: 0.026866\n",
      "Epoch 392, Train Loss: 0.026488, Val Loss: 0.026970\n",
      "Epoch 393, Train Loss: 0.026499, Val Loss: 0.026844\n",
      "Epoch 394, Train Loss: 0.026523, Val Loss: 0.026862\n",
      "Epoch 395, Train Loss: 0.026497, Val Loss: 0.026880\n",
      "Epoch 396, Train Loss: 0.026522, Val Loss: 0.026856\n",
      "Epoch 397, Train Loss: 0.026594, Val Loss: 0.026814\n",
      "Epoch 398, Train Loss: 0.026475, Val Loss: 0.026894\n",
      "Epoch 399, Train Loss: 0.026507, Val Loss: 0.026827\n",
      "Epoch 400, Train Loss: 0.026465, Val Loss: 0.026927\n",
      "Epoch 401, Train Loss: 0.026537, Val Loss: 0.026813\n",
      "Epoch 402, Train Loss: 0.026418, Val Loss: 0.026919\n",
      "Epoch 403, Train Loss: 0.026421, Val Loss: 0.026873\n",
      "Epoch 404, Train Loss: 0.026464, Val Loss: 0.026847\n",
      "Epoch 405, Train Loss: 0.026510, Val Loss: 0.026856\n",
      "Epoch 406, Train Loss: 0.026447, Val Loss: 0.027098\n",
      "Epoch 407, Train Loss: 0.026447, Val Loss: 0.026902\n",
      "Epoch 408, Train Loss: 0.026371, Val Loss: 0.026722\n",
      "Epoch 409, Train Loss: 0.026414, Val Loss: 0.026783\n",
      "Epoch 410, Train Loss: 0.026478, Val Loss: 0.026722\n",
      "Epoch 411, Train Loss: 0.026448, Val Loss: 0.026691\n",
      "Epoch 412, Train Loss: 0.026401, Val Loss: 0.026960\n",
      "Epoch 413, Train Loss: 0.026434, Val Loss: 0.026699\n",
      "Epoch 414, Train Loss: 0.026436, Val Loss: 0.026694\n",
      "Epoch 415, Train Loss: 0.026364, Val Loss: 0.026800\n",
      "Epoch 416, Train Loss: 0.026403, Val Loss: 0.026788\n",
      "Epoch 417, Train Loss: 0.026395, Val Loss: 0.026696\n",
      "Epoch 418, Train Loss: 0.026466, Val Loss: 0.026815\n",
      "Epoch 419, Train Loss: 0.026401, Val Loss: 0.026991\n",
      "Epoch 420, Train Loss: 0.026328, Val Loss: 0.026726\n",
      "Epoch 421, Train Loss: 0.026388, Val Loss: 0.026714\n",
      "Epoch 422, Train Loss: 0.026344, Val Loss: 0.026677\n",
      "Epoch 423, Train Loss: 0.026357, Val Loss: 0.026788\n",
      "Epoch 424, Train Loss: 0.026440, Val Loss: 0.026669\n",
      "Epoch 425, Train Loss: 0.026350, Val Loss: 0.026831\n",
      "Epoch 426, Train Loss: 0.026356, Val Loss: 0.026753\n",
      "Epoch 427, Train Loss: 0.026396, Val Loss: 0.026581\n",
      "Epoch 428, Train Loss: 0.026318, Val Loss: 0.026653\n",
      "Epoch 429, Train Loss: 0.026309, Val Loss: 0.026787\n",
      "Epoch 430, Train Loss: 0.026260, Val Loss: 0.026762\n",
      "Epoch 431, Train Loss: 0.026297, Val Loss: 0.026988\n",
      "Epoch 432, Train Loss: 0.026286, Val Loss: 0.026778\n",
      "Epoch 433, Train Loss: 0.026347, Val Loss: 0.026671\n",
      "Epoch 434, Train Loss: 0.026342, Val Loss: 0.026690\n",
      "Epoch 435, Train Loss: 0.026272, Val Loss: 0.026710\n",
      "Epoch 436, Train Loss: 0.026294, Val Loss: 0.026745\n",
      "Epoch 437, Train Loss: 0.026279, Val Loss: 0.026532\n",
      "Epoch 438, Train Loss: 0.026265, Val Loss: 0.026534\n",
      "Epoch 439, Train Loss: 0.026249, Val Loss: 0.026751\n",
      "Epoch 440, Train Loss: 0.026228, Val Loss: 0.026659\n",
      "Epoch 441, Train Loss: 0.026271, Val Loss: 0.026499\n",
      "Epoch 442, Train Loss: 0.026252, Val Loss: 0.026602\n",
      "Epoch 443, Train Loss: 0.026214, Val Loss: 0.026534\n",
      "Epoch 444, Train Loss: 0.026329, Val Loss: 0.026475\n",
      "Epoch 445, Train Loss: 0.026236, Val Loss: 0.026508\n",
      "Epoch 446, Train Loss: 0.026242, Val Loss: 0.026587\n",
      "Epoch 447, Train Loss: 0.026208, Val Loss: 0.026647\n",
      "Epoch 448, Train Loss: 0.026207, Val Loss: 0.026620\n",
      "Epoch 449, Train Loss: 0.026261, Val Loss: 0.026838\n",
      "Epoch 450, Train Loss: 0.026252, Val Loss: 0.026437\n",
      "Epoch 451, Train Loss: 0.026212, Val Loss: 0.026531\n",
      "Epoch 452, Train Loss: 0.026289, Val Loss: 0.026540\n",
      "Epoch 453, Train Loss: 0.026193, Val Loss: 0.026499\n",
      "Epoch 454, Train Loss: 0.026224, Val Loss: 0.026784\n",
      "Epoch 455, Train Loss: 0.026218, Val Loss: 0.026475\n",
      "Epoch 456, Train Loss: 0.026191, Val Loss: 0.026413\n",
      "Epoch 457, Train Loss: 0.026180, Val Loss: 0.026540\n",
      "Epoch 458, Train Loss: 0.026285, Val Loss: 0.026510\n",
      "Epoch 459, Train Loss: 0.026166, Val Loss: 0.026853\n",
      "Epoch 460, Train Loss: 0.026208, Val Loss: 0.026439\n",
      "Epoch 461, Train Loss: 0.026186, Val Loss: 0.026406\n",
      "Epoch 462, Train Loss: 0.026170, Val Loss: 0.026937\n",
      "Epoch 463, Train Loss: 0.026168, Val Loss: 0.026528\n",
      "Epoch 464, Train Loss: 0.026153, Val Loss: 0.026678\n",
      "Epoch 465, Train Loss: 0.026147, Val Loss: 0.026377\n",
      "Epoch 466, Train Loss: 0.026179, Val Loss: 0.026554\n",
      "Epoch 467, Train Loss: 0.026191, Val Loss: 0.026505\n",
      "Epoch 468, Train Loss: 0.026157, Val Loss: 0.026326\n",
      "Epoch 469, Train Loss: 0.026110, Val Loss: 0.026371\n",
      "Epoch 470, Train Loss: 0.026106, Val Loss: 0.026308\n",
      "Epoch 471, Train Loss: 0.026074, Val Loss: 0.026583\n",
      "Epoch 472, Train Loss: 0.026091, Val Loss: 0.026465\n",
      "Epoch 473, Train Loss: 0.026091, Val Loss: 0.026380\n",
      "Epoch 474, Train Loss: 0.026147, Val Loss: 0.026456\n",
      "Epoch 475, Train Loss: 0.026112, Val Loss: 0.026302\n",
      "Epoch 476, Train Loss: 0.026124, Val Loss: 0.026587\n",
      "Epoch 477, Train Loss: 0.026123, Val Loss: 0.026328\n",
      "Epoch 478, Train Loss: 0.026082, Val Loss: 0.026256\n",
      "Epoch 479, Train Loss: 0.026182, Val Loss: 0.026230\n",
      "Epoch 480, Train Loss: 0.026134, Val Loss: 0.026354\n",
      "Epoch 481, Train Loss: 0.026037, Val Loss: 0.026316\n",
      "Epoch 482, Train Loss: 0.025990, Val Loss: 0.026641\n",
      "Epoch 483, Train Loss: 0.026040, Val Loss: 0.026184\n",
      "Epoch 484, Train Loss: 0.026011, Val Loss: 0.026281\n",
      "Epoch 485, Train Loss: 0.026049, Val Loss: 0.026227\n",
      "Epoch 486, Train Loss: 0.025960, Val Loss: 0.026202\n",
      "Epoch 487, Train Loss: 0.026008, Val Loss: 0.026203\n",
      "Epoch 488, Train Loss: 0.025957, Val Loss: 0.026214\n",
      "Epoch 489, Train Loss: 0.025993, Val Loss: 0.026118\n",
      "Epoch 490, Train Loss: 0.025992, Val Loss: 0.026165\n",
      "Epoch 491, Train Loss: 0.025974, Val Loss: 0.026395\n",
      "Epoch 492, Train Loss: 0.025954, Val Loss: 0.026278\n",
      "Epoch 493, Train Loss: 0.025984, Val Loss: 0.026413\n",
      "Epoch 494, Train Loss: 0.025958, Val Loss: 0.026124\n",
      "Epoch 495, Train Loss: 0.025915, Val Loss: 0.026280\n",
      "Epoch 496, Train Loss: 0.025969, Val Loss: 0.026364\n",
      "Epoch 497, Train Loss: 0.025922, Val Loss: 0.026160\n",
      "Epoch 498, Train Loss: 0.025952, Val Loss: 0.026204\n",
      "Epoch 499, Train Loss: 0.025940, Val Loss: 0.026166\n",
      "Epoch 500, Train Loss: 0.025875, Val Loss: 0.026120\n",
      "Epoch 501, Train Loss: 0.025900, Val Loss: 0.026157\n",
      "Epoch 502, Train Loss: 0.025912, Val Loss: 0.026160\n",
      "Epoch 503, Train Loss: 0.025919, Val Loss: 0.026250\n",
      "Epoch 504, Train Loss: 0.025846, Val Loss: 0.026072\n",
      "Epoch 505, Train Loss: 0.025862, Val Loss: 0.026133\n",
      "Epoch 506, Train Loss: 0.025863, Val Loss: 0.026260\n",
      "Epoch 507, Train Loss: 0.025894, Val Loss: 0.026077\n",
      "Epoch 508, Train Loss: 0.025800, Val Loss: 0.025967\n",
      "Epoch 509, Train Loss: 0.025845, Val Loss: 0.026318\n",
      "Epoch 510, Train Loss: 0.025929, Val Loss: 0.025943\n",
      "Epoch 511, Train Loss: 0.025840, Val Loss: 0.026112\n",
      "Epoch 512, Train Loss: 0.025844, Val Loss: 0.025962\n",
      "Epoch 513, Train Loss: 0.025737, Val Loss: 0.025935\n",
      "Epoch 514, Train Loss: 0.025838, Val Loss: 0.026005\n",
      "Epoch 515, Train Loss: 0.025849, Val Loss: 0.026146\n",
      "Epoch 516, Train Loss: 0.025843, Val Loss: 0.025903\n",
      "Epoch 517, Train Loss: 0.025787, Val Loss: 0.025974\n",
      "Epoch 518, Train Loss: 0.025754, Val Loss: 0.025951\n",
      "Epoch 519, Train Loss: 0.025724, Val Loss: 0.025866\n",
      "Epoch 520, Train Loss: 0.025728, Val Loss: 0.025913\n",
      "Epoch 521, Train Loss: 0.025736, Val Loss: 0.025882\n",
      "Epoch 522, Train Loss: 0.025681, Val Loss: 0.025992\n",
      "Epoch 523, Train Loss: 0.025722, Val Loss: 0.025932\n",
      "Epoch 524, Train Loss: 0.025687, Val Loss: 0.025925\n",
      "Epoch 525, Train Loss: 0.025808, Val Loss: 0.025997\n",
      "Epoch 526, Train Loss: 0.025712, Val Loss: 0.026166\n",
      "Epoch 527, Train Loss: 0.025698, Val Loss: 0.025779\n",
      "Epoch 528, Train Loss: 0.025763, Val Loss: 0.025826\n",
      "Epoch 529, Train Loss: 0.025752, Val Loss: 0.025767\n",
      "Epoch 530, Train Loss: 0.025657, Val Loss: 0.025895\n",
      "Epoch 531, Train Loss: 0.025615, Val Loss: 0.025895\n",
      "Epoch 532, Train Loss: 0.025731, Val Loss: 0.025753\n",
      "Epoch 533, Train Loss: 0.025629, Val Loss: 0.025872\n",
      "Epoch 534, Train Loss: 0.025733, Val Loss: 0.025858\n",
      "Epoch 535, Train Loss: 0.025711, Val Loss: 0.025906\n",
      "Epoch 536, Train Loss: 0.025663, Val Loss: 0.025575\n",
      "Epoch 537, Train Loss: 0.025561, Val Loss: 0.025726\n",
      "Epoch 538, Train Loss: 0.025584, Val Loss: 0.025776\n",
      "Epoch 539, Train Loss: 0.025613, Val Loss: 0.025597\n",
      "Epoch 540, Train Loss: 0.025651, Val Loss: 0.025623\n",
      "Epoch 541, Train Loss: 0.025632, Val Loss: 0.025650\n",
      "Epoch 542, Train Loss: 0.025576, Val Loss: 0.025516\n",
      "Epoch 543, Train Loss: 0.025611, Val Loss: 0.025672\n",
      "Epoch 544, Train Loss: 0.025574, Val Loss: 0.025584\n",
      "Epoch 545, Train Loss: 0.025597, Val Loss: 0.025806\n",
      "Epoch 546, Train Loss: 0.025526, Val Loss: 0.025512\n",
      "Epoch 547, Train Loss: 0.025548, Val Loss: 0.025450\n",
      "Epoch 548, Train Loss: 0.025467, Val Loss: 0.025661\n",
      "Epoch 549, Train Loss: 0.025512, Val Loss: 0.025664\n",
      "Epoch 550, Train Loss: 0.025537, Val Loss: 0.025642\n",
      "Epoch 551, Train Loss: 0.025458, Val Loss: 0.025633\n",
      "Epoch 552, Train Loss: 0.025462, Val Loss: 0.025395\n",
      "Epoch 553, Train Loss: 0.025571, Val Loss: 0.025597\n",
      "Epoch 554, Train Loss: 0.025564, Val Loss: 0.025553\n",
      "Epoch 555, Train Loss: 0.025425, Val Loss: 0.025593\n",
      "Epoch 556, Train Loss: 0.025502, Val Loss: 0.025384\n",
      "Epoch 557, Train Loss: 0.025506, Val Loss: 0.025428\n",
      "Epoch 558, Train Loss: 0.025451, Val Loss: 0.025353\n",
      "Epoch 559, Train Loss: 0.025409, Val Loss: 0.025406\n",
      "Epoch 560, Train Loss: 0.025451, Val Loss: 0.025330\n",
      "Epoch 561, Train Loss: 0.025438, Val Loss: 0.025374\n",
      "Epoch 562, Train Loss: 0.025427, Val Loss: 0.025265\n",
      "Epoch 563, Train Loss: 0.025417, Val Loss: 0.025444\n",
      "Epoch 564, Train Loss: 0.025384, Val Loss: 0.025440\n",
      "Epoch 565, Train Loss: 0.025432, Val Loss: 0.025253\n",
      "Epoch 566, Train Loss: 0.025358, Val Loss: 0.025365\n",
      "Epoch 567, Train Loss: 0.025329, Val Loss: 0.025343\n",
      "Epoch 568, Train Loss: 0.025398, Val Loss: 0.025162\n",
      "Epoch 569, Train Loss: 0.025312, Val Loss: 0.025252\n",
      "Epoch 570, Train Loss: 0.025298, Val Loss: 0.025129\n",
      "Epoch 571, Train Loss: 0.025341, Val Loss: 0.025176\n",
      "Epoch 572, Train Loss: 0.025304, Val Loss: 0.025107\n",
      "Epoch 573, Train Loss: 0.025285, Val Loss: 0.025157\n",
      "Epoch 574, Train Loss: 0.025223, Val Loss: 0.025107\n",
      "Epoch 575, Train Loss: 0.025271, Val Loss: 0.025090\n",
      "Epoch 576, Train Loss: 0.025318, Val Loss: 0.025288\n",
      "Epoch 577, Train Loss: 0.025270, Val Loss: 0.025379\n",
      "Epoch 578, Train Loss: 0.025285, Val Loss: 0.025056\n",
      "Epoch 579, Train Loss: 0.025241, Val Loss: 0.024992\n",
      "Epoch 580, Train Loss: 0.025198, Val Loss: 0.025272\n",
      "Epoch 581, Train Loss: 0.025252, Val Loss: 0.025267\n",
      "Epoch 582, Train Loss: 0.025207, Val Loss: 0.024951\n",
      "Epoch 583, Train Loss: 0.025189, Val Loss: 0.025208\n",
      "Epoch 584, Train Loss: 0.025227, Val Loss: 0.024905\n",
      "Epoch 585, Train Loss: 0.025187, Val Loss: 0.024893\n",
      "Epoch 586, Train Loss: 0.025160, Val Loss: 0.025004\n",
      "Epoch 587, Train Loss: 0.025146, Val Loss: 0.024837\n",
      "Epoch 588, Train Loss: 0.025138, Val Loss: 0.024960\n",
      "Epoch 589, Train Loss: 0.025192, Val Loss: 0.024857\n",
      "Epoch 590, Train Loss: 0.025069, Val Loss: 0.024869\n",
      "Epoch 591, Train Loss: 0.025135, Val Loss: 0.024874\n",
      "Epoch 592, Train Loss: 0.025081, Val Loss: 0.024985\n",
      "Epoch 593, Train Loss: 0.025063, Val Loss: 0.024822\n",
      "Epoch 594, Train Loss: 0.025033, Val Loss: 0.024713\n",
      "Epoch 595, Train Loss: 0.024963, Val Loss: 0.024854\n",
      "Epoch 596, Train Loss: 0.025122, Val Loss: 0.024728\n",
      "Epoch 597, Train Loss: 0.025066, Val Loss: 0.024729\n",
      "Epoch 598, Train Loss: 0.024983, Val Loss: 0.024762\n",
      "Epoch 599, Train Loss: 0.025014, Val Loss: 0.024690\n",
      "Epoch 600, Train Loss: 0.025070, Val Loss: 0.024752\n",
      "Epoch 601, Train Loss: 0.024945, Val Loss: 0.024807\n",
      "Epoch 602, Train Loss: 0.025012, Val Loss: 0.024674\n",
      "Epoch 603, Train Loss: 0.025041, Val Loss: 0.024866\n",
      "Epoch 604, Train Loss: 0.024954, Val Loss: 0.024765\n",
      "Epoch 605, Train Loss: 0.025040, Val Loss: 0.024683\n",
      "Epoch 606, Train Loss: 0.024888, Val Loss: 0.024514\n",
      "Epoch 607, Train Loss: 0.025025, Val Loss: 0.024777\n",
      "Epoch 608, Train Loss: 0.024876, Val Loss: 0.024648\n",
      "Epoch 609, Train Loss: 0.024970, Val Loss: 0.024521\n",
      "Epoch 610, Train Loss: 0.024913, Val Loss: 0.024610\n",
      "Epoch 611, Train Loss: 0.024956, Val Loss: 0.024587\n",
      "Epoch 612, Train Loss: 0.024900, Val Loss: 0.024621\n",
      "Epoch 613, Train Loss: 0.024908, Val Loss: 0.024744\n",
      "Epoch 614, Train Loss: 0.024930, Val Loss: 0.024353\n",
      "Epoch 615, Train Loss: 0.024850, Val Loss: 0.024538\n",
      "Epoch 616, Train Loss: 0.024808, Val Loss: 0.024638\n",
      "Epoch 617, Train Loss: 0.024922, Val Loss: 0.024457\n",
      "Epoch 618, Train Loss: 0.024910, Val Loss: 0.024469\n",
      "Epoch 619, Train Loss: 0.024861, Val Loss: 0.024399\n",
      "Epoch 620, Train Loss: 0.024858, Val Loss: 0.024272\n",
      "Epoch 621, Train Loss: 0.024756, Val Loss: 0.024351\n",
      "Epoch 622, Train Loss: 0.024813, Val Loss: 0.024513\n",
      "Epoch 623, Train Loss: 0.024769, Val Loss: 0.024396\n",
      "Epoch 624, Train Loss: 0.024825, Val Loss: 0.024339\n",
      "Epoch 625, Train Loss: 0.024773, Val Loss: 0.024316\n",
      "Epoch 626, Train Loss: 0.024725, Val Loss: 0.024242\n",
      "Epoch 627, Train Loss: 0.024707, Val Loss: 0.024264\n",
      "Epoch 628, Train Loss: 0.024743, Val Loss: 0.024297\n",
      "Epoch 629, Train Loss: 0.024724, Val Loss: 0.024268\n",
      "Epoch 630, Train Loss: 0.024718, Val Loss: 0.024274\n",
      "Epoch 631, Train Loss: 0.024793, Val Loss: 0.024357\n",
      "Epoch 632, Train Loss: 0.024710, Val Loss: 0.024247\n",
      "Epoch 633, Train Loss: 0.024673, Val Loss: 0.024227\n",
      "Epoch 634, Train Loss: 0.024700, Val Loss: 0.024490\n",
      "Epoch 635, Train Loss: 0.024669, Val Loss: 0.024476\n",
      "Epoch 636, Train Loss: 0.024736, Val Loss: 0.024391\n",
      "Epoch 637, Train Loss: 0.024632, Val Loss: 0.024190\n",
      "Epoch 638, Train Loss: 0.024692, Val Loss: 0.024308\n",
      "Epoch 639, Train Loss: 0.024610, Val Loss: 0.024224\n",
      "Epoch 640, Train Loss: 0.024604, Val Loss: 0.024444\n",
      "Epoch 641, Train Loss: 0.024631, Val Loss: 0.024258\n",
      "Epoch 642, Train Loss: 0.024607, Val Loss: 0.024107\n",
      "Epoch 643, Train Loss: 0.024606, Val Loss: 0.023983\n",
      "Epoch 644, Train Loss: 0.024674, Val Loss: 0.024279\n",
      "Epoch 645, Train Loss: 0.024607, Val Loss: 0.024130\n",
      "Epoch 646, Train Loss: 0.024572, Val Loss: 0.024117\n",
      "Epoch 647, Train Loss: 0.024485, Val Loss: 0.024278\n",
      "Epoch 648, Train Loss: 0.024532, Val Loss: 0.024163\n",
      "Epoch 649, Train Loss: 0.024510, Val Loss: 0.024030\n",
      "Epoch 650, Train Loss: 0.024566, Val Loss: 0.024052\n",
      "Epoch 651, Train Loss: 0.024575, Val Loss: 0.024100\n",
      "Epoch 652, Train Loss: 0.024590, Val Loss: 0.024107\n",
      "Epoch 653, Train Loss: 0.024520, Val Loss: 0.023987\n",
      "Epoch 654, Train Loss: 0.024527, Val Loss: 0.024161\n",
      "Epoch 655, Train Loss: 0.024477, Val Loss: 0.023999\n",
      "Epoch 656, Train Loss: 0.024459, Val Loss: 0.023957\n",
      "Epoch 657, Train Loss: 0.024432, Val Loss: 0.024041\n",
      "Epoch 658, Train Loss: 0.024492, Val Loss: 0.024219\n",
      "Epoch 659, Train Loss: 0.024548, Val Loss: 0.024262\n",
      "Epoch 660, Train Loss: 0.024420, Val Loss: 0.024131\n",
      "Epoch 661, Train Loss: 0.024370, Val Loss: 0.023887\n",
      "Epoch 662, Train Loss: 0.024395, Val Loss: 0.023958\n",
      "Epoch 663, Train Loss: 0.024455, Val Loss: 0.024094\n",
      "Epoch 664, Train Loss: 0.024429, Val Loss: 0.024142\n",
      "Epoch 665, Train Loss: 0.024505, Val Loss: 0.023939\n",
      "Epoch 666, Train Loss: 0.024412, Val Loss: 0.023865\n",
      "Epoch 667, Train Loss: 0.024411, Val Loss: 0.024105\n",
      "Epoch 668, Train Loss: 0.024348, Val Loss: 0.023837\n",
      "Epoch 669, Train Loss: 0.024369, Val Loss: 0.023881\n",
      "Epoch 670, Train Loss: 0.024390, Val Loss: 0.023843\n",
      "Epoch 671, Train Loss: 0.024302, Val Loss: 0.023837\n",
      "Epoch 672, Train Loss: 0.024329, Val Loss: 0.023914\n",
      "Epoch 673, Train Loss: 0.024406, Val Loss: 0.023806\n",
      "Epoch 674, Train Loss: 0.024240, Val Loss: 0.023883\n",
      "Epoch 675, Train Loss: 0.024282, Val Loss: 0.023778\n",
      "Epoch 676, Train Loss: 0.024271, Val Loss: 0.023741\n",
      "Epoch 677, Train Loss: 0.024290, Val Loss: 0.023712\n",
      "Epoch 678, Train Loss: 0.024287, Val Loss: 0.023883\n",
      "Epoch 679, Train Loss: 0.024405, Val Loss: 0.024212\n",
      "Epoch 680, Train Loss: 0.024297, Val Loss: 0.023726\n",
      "Epoch 681, Train Loss: 0.024252, Val Loss: 0.023904\n",
      "Epoch 682, Train Loss: 0.024226, Val Loss: 0.023957\n",
      "Epoch 683, Train Loss: 0.024365, Val Loss: 0.023945\n",
      "Epoch 684, Train Loss: 0.024190, Val Loss: 0.023657\n",
      "Epoch 685, Train Loss: 0.024253, Val Loss: 0.023633\n",
      "Epoch 686, Train Loss: 0.024224, Val Loss: 0.023757\n",
      "Epoch 687, Train Loss: 0.024162, Val Loss: 0.023749\n",
      "Epoch 688, Train Loss: 0.024157, Val Loss: 0.023700\n",
      "Epoch 689, Train Loss: 0.024225, Val Loss: 0.023939\n",
      "Epoch 690, Train Loss: 0.024193, Val Loss: 0.023734\n",
      "Epoch 691, Train Loss: 0.024218, Val Loss: 0.023667\n",
      "Epoch 692, Train Loss: 0.024199, Val Loss: 0.023701\n",
      "Epoch 693, Train Loss: 0.024216, Val Loss: 0.023724\n",
      "Epoch 694, Train Loss: 0.024150, Val Loss: 0.023674\n",
      "Epoch 695, Train Loss: 0.024064, Val Loss: 0.023579\n",
      "Epoch 696, Train Loss: 0.024221, Val Loss: 0.023617\n",
      "Epoch 697, Train Loss: 0.024166, Val Loss: 0.023695\n",
      "Epoch 698, Train Loss: 0.024169, Val Loss: 0.023615\n",
      "Epoch 699, Train Loss: 0.024092, Val Loss: 0.023493\n",
      "Epoch 700, Train Loss: 0.024136, Val Loss: 0.023638\n",
      "Epoch 701, Train Loss: 0.024063, Val Loss: 0.023666\n",
      "Epoch 702, Train Loss: 0.024028, Val Loss: 0.023506\n",
      "Epoch 703, Train Loss: 0.024179, Val Loss: 0.023636\n",
      "Epoch 704, Train Loss: 0.024171, Val Loss: 0.023574\n",
      "Epoch 705, Train Loss: 0.024077, Val Loss: 0.023767\n",
      "Epoch 706, Train Loss: 0.024121, Val Loss: 0.023504\n",
      "Epoch 707, Train Loss: 0.024077, Val Loss: 0.023574\n",
      "Epoch 708, Train Loss: 0.024096, Val Loss: 0.023802\n",
      "Epoch 709, Train Loss: 0.024032, Val Loss: 0.023670\n",
      "Epoch 710, Train Loss: 0.024066, Val Loss: 0.023812\n",
      "Epoch 711, Train Loss: 0.024078, Val Loss: 0.023566\n",
      "Epoch 712, Train Loss: 0.024060, Val Loss: 0.023644\n",
      "Epoch 713, Train Loss: 0.024068, Val Loss: 0.023486\n",
      "Epoch 714, Train Loss: 0.024049, Val Loss: 0.023545\n",
      "Epoch 715, Train Loss: 0.024042, Val Loss: 0.023697\n",
      "Epoch 716, Train Loss: 0.024068, Val Loss: 0.023517\n",
      "Epoch 717, Train Loss: 0.024137, Val Loss: 0.023443\n",
      "Epoch 718, Train Loss: 0.024087, Val Loss: 0.023743\n",
      "Epoch 719, Train Loss: 0.023936, Val Loss: 0.023525\n",
      "Epoch 720, Train Loss: 0.024022, Val Loss: 0.023432\n",
      "Epoch 721, Train Loss: 0.023934, Val Loss: 0.023646\n",
      "Epoch 722, Train Loss: 0.023966, Val Loss: 0.023892\n",
      "Epoch 723, Train Loss: 0.023950, Val Loss: 0.023412\n",
      "Epoch 724, Train Loss: 0.024084, Val Loss: 0.023505\n",
      "Epoch 725, Train Loss: 0.023982, Val Loss: 0.023574\n",
      "Epoch 726, Train Loss: 0.023980, Val Loss: 0.023445\n",
      "Epoch 727, Train Loss: 0.024001, Val Loss: 0.023638\n",
      "Epoch 728, Train Loss: 0.024135, Val Loss: 0.023553\n",
      "Epoch 729, Train Loss: 0.023913, Val Loss: 0.023572\n",
      "Epoch 730, Train Loss: 0.023897, Val Loss: 0.023723\n",
      "Epoch 731, Train Loss: 0.023970, Val Loss: 0.023334\n",
      "Epoch 732, Train Loss: 0.023981, Val Loss: 0.023276\n",
      "Epoch 733, Train Loss: 0.023897, Val Loss: 0.023534\n",
      "Epoch 734, Train Loss: 0.024046, Val Loss: 0.023325\n",
      "Epoch 735, Train Loss: 0.023933, Val Loss: 0.023436\n",
      "Epoch 736, Train Loss: 0.023900, Val Loss: 0.023305\n",
      "Epoch 737, Train Loss: 0.023928, Val Loss: 0.023360\n",
      "Epoch 738, Train Loss: 0.023881, Val Loss: 0.023562\n",
      "Epoch 739, Train Loss: 0.023818, Val Loss: 0.023407\n",
      "Epoch 740, Train Loss: 0.023902, Val Loss: 0.023426\n",
      "Epoch 741, Train Loss: 0.023938, Val Loss: 0.023599\n",
      "Epoch 742, Train Loss: 0.023980, Val Loss: 0.023437\n",
      "Epoch 743, Train Loss: 0.023891, Val Loss: 0.023513\n",
      "Epoch 744, Train Loss: 0.023927, Val Loss: 0.023272\n",
      "Epoch 745, Train Loss: 0.023840, Val Loss: 0.023357\n",
      "Epoch 746, Train Loss: 0.023976, Val Loss: 0.023364\n",
      "Epoch 747, Train Loss: 0.023897, Val Loss: 0.023377\n",
      "Epoch 748, Train Loss: 0.023776, Val Loss: 0.023442\n",
      "Epoch 749, Train Loss: 0.023805, Val Loss: 0.023771\n",
      "Epoch 750, Train Loss: 0.023819, Val Loss: 0.023326\n",
      "Epoch 751, Train Loss: 0.023838, Val Loss: 0.023282\n",
      "Epoch 752, Train Loss: 0.023787, Val Loss: 0.023425\n",
      "Epoch 753, Train Loss: 0.023961, Val Loss: 0.023315\n",
      "Epoch 754, Train Loss: 0.023810, Val Loss: 0.023190\n",
      "Epoch 755, Train Loss: 0.023769, Val Loss: 0.023233\n",
      "Epoch 756, Train Loss: 0.023801, Val Loss: 0.023724\n",
      "Epoch 757, Train Loss: 0.023814, Val Loss: 0.023389\n",
      "Epoch 758, Train Loss: 0.023783, Val Loss: 0.023380\n",
      "Epoch 759, Train Loss: 0.023784, Val Loss: 0.023287\n",
      "Epoch 760, Train Loss: 0.023783, Val Loss: 0.023307\n",
      "Epoch 761, Train Loss: 0.023768, Val Loss: 0.023161\n",
      "Epoch 762, Train Loss: 0.023780, Val Loss: 0.023562\n",
      "Epoch 763, Train Loss: 0.023811, Val Loss: 0.023310\n",
      "Epoch 764, Train Loss: 0.023775, Val Loss: 0.023243\n",
      "Epoch 765, Train Loss: 0.023847, Val Loss: 0.023179\n",
      "Epoch 766, Train Loss: 0.023756, Val Loss: 0.023204\n",
      "Epoch 767, Train Loss: 0.023719, Val Loss: 0.023239\n",
      "Epoch 768, Train Loss: 0.023747, Val Loss: 0.023137\n",
      "Epoch 769, Train Loss: 0.023847, Val Loss: 0.023210\n",
      "Epoch 770, Train Loss: 0.023782, Val Loss: 0.023310\n",
      "Epoch 771, Train Loss: 0.023821, Val Loss: 0.023304\n",
      "Epoch 772, Train Loss: 0.023743, Val Loss: 0.023434\n",
      "Epoch 773, Train Loss: 0.023769, Val Loss: 0.023445\n",
      "Epoch 774, Train Loss: 0.023663, Val Loss: 0.023200\n",
      "Epoch 775, Train Loss: 0.023690, Val Loss: 0.023272\n",
      "Epoch 776, Train Loss: 0.023734, Val Loss: 0.023166\n",
      "Epoch 777, Train Loss: 0.023712, Val Loss: 0.023324\n",
      "Epoch 778, Train Loss: 0.023750, Val Loss: 0.023073\n",
      "Epoch 779, Train Loss: 0.023711, Val Loss: 0.023281\n",
      "Epoch 780, Train Loss: 0.023712, Val Loss: 0.023262\n",
      "Epoch 781, Train Loss: 0.023726, Val Loss: 0.023254\n",
      "Epoch 782, Train Loss: 0.023761, Val Loss: 0.023369\n",
      "Epoch 783, Train Loss: 0.023680, Val Loss: 0.023224\n",
      "Epoch 784, Train Loss: 0.023622, Val Loss: 0.023127\n",
      "Epoch 785, Train Loss: 0.023635, Val Loss: 0.023365\n",
      "Epoch 786, Train Loss: 0.023669, Val Loss: 0.023149\n",
      "Epoch 787, Train Loss: 0.023755, Val Loss: 0.023219\n",
      "Epoch 788, Train Loss: 0.023683, Val Loss: 0.023324\n",
      "Epoch 789, Train Loss: 0.023726, Val Loss: 0.023257\n",
      "Epoch 790, Train Loss: 0.023729, Val Loss: 0.023124\n",
      "Epoch 791, Train Loss: 0.023738, Val Loss: 0.023191\n",
      "Epoch 792, Train Loss: 0.023630, Val Loss: 0.023025\n",
      "Epoch 793, Train Loss: 0.023628, Val Loss: 0.023057\n",
      "Epoch 794, Train Loss: 0.023674, Val Loss: 0.023326\n",
      "Epoch 795, Train Loss: 0.023634, Val Loss: 0.023225\n",
      "Epoch 796, Train Loss: 0.023617, Val Loss: 0.023314\n",
      "Epoch 797, Train Loss: 0.023614, Val Loss: 0.023060\n",
      "Epoch 798, Train Loss: 0.023647, Val Loss: 0.023048\n",
      "Epoch 799, Train Loss: 0.023616, Val Loss: 0.023460\n",
      "Epoch 800, Train Loss: 0.023733, Val Loss: 0.023146\n",
      "Epoch 801, Train Loss: 0.023693, Val Loss: 0.023275\n",
      "Epoch 802, Train Loss: 0.023551, Val Loss: 0.023265\n",
      "Epoch 803, Train Loss: 0.023561, Val Loss: 0.023021\n",
      "Epoch 804, Train Loss: 0.023558, Val Loss: 0.023035\n",
      "Epoch 805, Train Loss: 0.023632, Val Loss: 0.023274\n",
      "Epoch 806, Train Loss: 0.023684, Val Loss: 0.023301\n",
      "Epoch 807, Train Loss: 0.023621, Val Loss: 0.023395\n",
      "Epoch 808, Train Loss: 0.023588, Val Loss: 0.023245\n",
      "Epoch 809, Train Loss: 0.023529, Val Loss: 0.023164\n",
      "Epoch 810, Train Loss: 0.023602, Val Loss: 0.023111\n",
      "Epoch 811, Train Loss: 0.023579, Val Loss: 0.023153\n",
      "Epoch 812, Train Loss: 0.023524, Val Loss: 0.023276\n",
      "Epoch 813, Train Loss: 0.023536, Val Loss: 0.023279\n",
      "Epoch 814, Train Loss: 0.023591, Val Loss: 0.023098\n",
      "Epoch 815, Train Loss: 0.023505, Val Loss: 0.023259\n",
      "Epoch 816, Train Loss: 0.023572, Val Loss: 0.023122\n",
      "Epoch 817, Train Loss: 0.023550, Val Loss: 0.022993\n",
      "Epoch 818, Train Loss: 0.023479, Val Loss: 0.023092\n",
      "Epoch 819, Train Loss: 0.023501, Val Loss: 0.023023\n",
      "Epoch 820, Train Loss: 0.023540, Val Loss: 0.023116\n",
      "Epoch 821, Train Loss: 0.023418, Val Loss: 0.023133\n",
      "Epoch 822, Train Loss: 0.023538, Val Loss: 0.022887\n",
      "Epoch 823, Train Loss: 0.023539, Val Loss: 0.022959\n",
      "Epoch 824, Train Loss: 0.023515, Val Loss: 0.022886\n",
      "Epoch 825, Train Loss: 0.023542, Val Loss: 0.023161\n",
      "Epoch 826, Train Loss: 0.023478, Val Loss: 0.023261\n",
      "Epoch 827, Train Loss: 0.023459, Val Loss: 0.022990\n",
      "Epoch 828, Train Loss: 0.023454, Val Loss: 0.023109\n",
      "Epoch 829, Train Loss: 0.023582, Val Loss: 0.023033\n",
      "Epoch 830, Train Loss: 0.023412, Val Loss: 0.023218\n",
      "Epoch 831, Train Loss: 0.023458, Val Loss: 0.023054\n",
      "Epoch 832, Train Loss: 0.023429, Val Loss: 0.023035\n",
      "Epoch 833, Train Loss: 0.023443, Val Loss: 0.023142\n",
      "Epoch 834, Train Loss: 0.023490, Val Loss: 0.023035\n",
      "Epoch 835, Train Loss: 0.023522, Val Loss: 0.023146\n",
      "Epoch 836, Train Loss: 0.023507, Val Loss: 0.022963\n",
      "Epoch 837, Train Loss: 0.023402, Val Loss: 0.023344\n",
      "Epoch 838, Train Loss: 0.023493, Val Loss: 0.023054\n",
      "Epoch 839, Train Loss: 0.023428, Val Loss: 0.022971\n",
      "Epoch 840, Train Loss: 0.023480, Val Loss: 0.023051\n",
      "Epoch 841, Train Loss: 0.023586, Val Loss: 0.022941\n",
      "Epoch 842, Train Loss: 0.023434, Val Loss: 0.022892\n",
      "Epoch 843, Train Loss: 0.023425, Val Loss: 0.022908\n",
      "Epoch 844, Train Loss: 0.023387, Val Loss: 0.022937\n",
      "Epoch 845, Train Loss: 0.023475, Val Loss: 0.022991\n",
      "Epoch 846, Train Loss: 0.023440, Val Loss: 0.023449\n",
      "Epoch 847, Train Loss: 0.023473, Val Loss: 0.023062\n",
      "Epoch 848, Train Loss: 0.023482, Val Loss: 0.022960\n",
      "Epoch 849, Train Loss: 0.023427, Val Loss: 0.023015\n",
      "Epoch 850, Train Loss: 0.023304, Val Loss: 0.023162\n",
      "Epoch 851, Train Loss: 0.023373, Val Loss: 0.022913\n",
      "Epoch 852, Train Loss: 0.023369, Val Loss: 0.022885\n",
      "Epoch 853, Train Loss: 0.023387, Val Loss: 0.022857\n",
      "Epoch 854, Train Loss: 0.023487, Val Loss: 0.022881\n",
      "Epoch 855, Train Loss: 0.023305, Val Loss: 0.023055\n",
      "Epoch 856, Train Loss: 0.023350, Val Loss: 0.023037\n",
      "Epoch 857, Train Loss: 0.023373, Val Loss: 0.022866\n",
      "Epoch 858, Train Loss: 0.023500, Val Loss: 0.023042\n",
      "Epoch 859, Train Loss: 0.023385, Val Loss: 0.023101\n",
      "Epoch 860, Train Loss: 0.023316, Val Loss: 0.023051\n",
      "Epoch 861, Train Loss: 0.023457, Val Loss: 0.022852\n",
      "Epoch 862, Train Loss: 0.023368, Val Loss: 0.022954\n",
      "Epoch 863, Train Loss: 0.023308, Val Loss: 0.022996\n",
      "Epoch 864, Train Loss: 0.023325, Val Loss: 0.022999\n",
      "Epoch 865, Train Loss: 0.023313, Val Loss: 0.022911\n",
      "Epoch 866, Train Loss: 0.023331, Val Loss: 0.022979\n",
      "Epoch 867, Train Loss: 0.023334, Val Loss: 0.022958\n",
      "Epoch 868, Train Loss: 0.023333, Val Loss: 0.023040\n",
      "Epoch 869, Train Loss: 0.023294, Val Loss: 0.022862\n",
      "Epoch 870, Train Loss: 0.023383, Val Loss: 0.022947\n",
      "Epoch 871, Train Loss: 0.023299, Val Loss: 0.023013\n",
      "Epoch 872, Train Loss: 0.023343, Val Loss: 0.022987\n",
      "Epoch 873, Train Loss: 0.023349, Val Loss: 0.022960\n",
      "Epoch 874, Train Loss: 0.023301, Val Loss: 0.023394\n",
      "Epoch 875, Train Loss: 0.023268, Val Loss: 0.022983\n",
      "Epoch 876, Train Loss: 0.023297, Val Loss: 0.022950\n",
      "Epoch 877, Train Loss: 0.023329, Val Loss: 0.022830\n",
      "Epoch 878, Train Loss: 0.023266, Val Loss: 0.023168\n",
      "Epoch 879, Train Loss: 0.023383, Val Loss: 0.023000\n",
      "Epoch 880, Train Loss: 0.023308, Val Loss: 0.022971\n",
      "Epoch 881, Train Loss: 0.023426, Val Loss: 0.022876\n",
      "Epoch 882, Train Loss: 0.023305, Val Loss: 0.022906\n",
      "Epoch 883, Train Loss: 0.023287, Val Loss: 0.022861\n",
      "Epoch 884, Train Loss: 0.023283, Val Loss: 0.022954\n",
      "Epoch 885, Train Loss: 0.023243, Val Loss: 0.022779\n",
      "Epoch 886, Train Loss: 0.023218, Val Loss: 0.022876\n",
      "Epoch 887, Train Loss: 0.023351, Val Loss: 0.023099\n",
      "Epoch 888, Train Loss: 0.023376, Val Loss: 0.022847\n",
      "Epoch 889, Train Loss: 0.023228, Val Loss: 0.023006\n",
      "Epoch 890, Train Loss: 0.023252, Val Loss: 0.023094\n",
      "Epoch 891, Train Loss: 0.023310, Val Loss: 0.022805\n",
      "Epoch 892, Train Loss: 0.023364, Val Loss: 0.022934\n",
      "Epoch 893, Train Loss: 0.023298, Val Loss: 0.022835\n",
      "Epoch 894, Train Loss: 0.023256, Val Loss: 0.023217\n",
      "Epoch 895, Train Loss: 0.023250, Val Loss: 0.022733\n",
      "Epoch 896, Train Loss: 0.023198, Val Loss: 0.023081\n",
      "Epoch 897, Train Loss: 0.023286, Val Loss: 0.022953\n",
      "Epoch 898, Train Loss: 0.023232, Val Loss: 0.022951\n",
      "Epoch 899, Train Loss: 0.023189, Val Loss: 0.022887\n",
      "Epoch 900, Train Loss: 0.023160, Val Loss: 0.023120\n",
      "Epoch 901, Train Loss: 0.023250, Val Loss: 0.022874\n",
      "Epoch 902, Train Loss: 0.023191, Val Loss: 0.022758\n",
      "Epoch 903, Train Loss: 0.023202, Val Loss: 0.022737\n",
      "Epoch 904, Train Loss: 0.023121, Val Loss: 0.022793\n",
      "Epoch 905, Train Loss: 0.023142, Val Loss: 0.022927\n",
      "Epoch 906, Train Loss: 0.023159, Val Loss: 0.022677\n",
      "Epoch 907, Train Loss: 0.023182, Val Loss: 0.022663\n",
      "Epoch 908, Train Loss: 0.023134, Val Loss: 0.022821\n",
      "Epoch 909, Train Loss: 0.023254, Val Loss: 0.022982\n",
      "Epoch 910, Train Loss: 0.023121, Val Loss: 0.022922\n",
      "Epoch 911, Train Loss: 0.023263, Val Loss: 0.022809\n",
      "Epoch 912, Train Loss: 0.023176, Val Loss: 0.022872\n",
      "Epoch 913, Train Loss: 0.023127, Val Loss: 0.022906\n",
      "Epoch 914, Train Loss: 0.023090, Val Loss: 0.023090\n",
      "Epoch 915, Train Loss: 0.023157, Val Loss: 0.022678\n",
      "Epoch 916, Train Loss: 0.023129, Val Loss: 0.022908\n",
      "Epoch 917, Train Loss: 0.023181, Val Loss: 0.022590\n",
      "Epoch 918, Train Loss: 0.023108, Val Loss: 0.022789\n",
      "Epoch 919, Train Loss: 0.023112, Val Loss: 0.022649\n",
      "Epoch 920, Train Loss: 0.023167, Val Loss: 0.022667\n",
      "Epoch 921, Train Loss: 0.023060, Val Loss: 0.022739\n",
      "Epoch 922, Train Loss: 0.023101, Val Loss: 0.022874\n",
      "Epoch 923, Train Loss: 0.023138, Val Loss: 0.022806\n",
      "Epoch 924, Train Loss: 0.023123, Val Loss: 0.022826\n",
      "Epoch 925, Train Loss: 0.023038, Val Loss: 0.022729\n",
      "Epoch 926, Train Loss: 0.023023, Val Loss: 0.022705\n",
      "Epoch 927, Train Loss: 0.023091, Val Loss: 0.022862\n",
      "Epoch 928, Train Loss: 0.023134, Val Loss: 0.022627\n",
      "Epoch 929, Train Loss: 0.023085, Val Loss: 0.022800\n",
      "Epoch 930, Train Loss: 0.023096, Val Loss: 0.022765\n",
      "Epoch 931, Train Loss: 0.023178, Val Loss: 0.022746\n",
      "Epoch 932, Train Loss: 0.023089, Val Loss: 0.022662\n",
      "Epoch 933, Train Loss: 0.023031, Val Loss: 0.022734\n",
      "Epoch 934, Train Loss: 0.023070, Val Loss: 0.022826\n",
      "Epoch 935, Train Loss: 0.023084, Val Loss: 0.022701\n",
      "Epoch 936, Train Loss: 0.023002, Val Loss: 0.022672\n",
      "Epoch 937, Train Loss: 0.023086, Val Loss: 0.022694\n",
      "Epoch 938, Train Loss: 0.023129, Val Loss: 0.022700\n",
      "Epoch 939, Train Loss: 0.023112, Val Loss: 0.022749\n",
      "Epoch 940, Train Loss: 0.023124, Val Loss: 0.022691\n",
      "Epoch 941, Train Loss: 0.023020, Val Loss: 0.022662\n",
      "Epoch 942, Train Loss: 0.022952, Val Loss: 0.022543\n",
      "Epoch 943, Train Loss: 0.022985, Val Loss: 0.022624\n",
      "Epoch 944, Train Loss: 0.022975, Val Loss: 0.022860\n",
      "Epoch 945, Train Loss: 0.022931, Val Loss: 0.022849\n",
      "Epoch 946, Train Loss: 0.022945, Val Loss: 0.022528\n",
      "Epoch 947, Train Loss: 0.023087, Val Loss: 0.022605\n",
      "Epoch 948, Train Loss: 0.022973, Val Loss: 0.022634\n",
      "Epoch 949, Train Loss: 0.022993, Val Loss: 0.022757\n",
      "Epoch 950, Train Loss: 0.022975, Val Loss: 0.022730\n",
      "Epoch 951, Train Loss: 0.022945, Val Loss: 0.022803\n",
      "Epoch 952, Train Loss: 0.022972, Val Loss: 0.022795\n",
      "Epoch 953, Train Loss: 0.023020, Val Loss: 0.022670\n",
      "Epoch 954, Train Loss: 0.022968, Val Loss: 0.022441\n",
      "Epoch 955, Train Loss: 0.022904, Val Loss: 0.022829\n",
      "Epoch 956, Train Loss: 0.022949, Val Loss: 0.022492\n",
      "Epoch 957, Train Loss: 0.022913, Val Loss: 0.022489\n",
      "Epoch 958, Train Loss: 0.022911, Val Loss: 0.022652\n",
      "Epoch 959, Train Loss: 0.022878, Val Loss: 0.022449\n",
      "Epoch 960, Train Loss: 0.022911, Val Loss: 0.022841\n",
      "Epoch 961, Train Loss: 0.023013, Val Loss: 0.022577\n",
      "Epoch 962, Train Loss: 0.022982, Val Loss: 0.022588\n",
      "Epoch 963, Train Loss: 0.022888, Val Loss: 0.022385\n",
      "Epoch 964, Train Loss: 0.022844, Val Loss: 0.022569\n",
      "Epoch 965, Train Loss: 0.022894, Val Loss: 0.022372\n",
      "Epoch 966, Train Loss: 0.022948, Val Loss: 0.022545\n",
      "Epoch 967, Train Loss: 0.022845, Val Loss: 0.022485\n",
      "Epoch 968, Train Loss: 0.022908, Val Loss: 0.022421\n",
      "Epoch 969, Train Loss: 0.022895, Val Loss: 0.022560\n",
      "Epoch 970, Train Loss: 0.022861, Val Loss: 0.022618\n",
      "Epoch 971, Train Loss: 0.022920, Val Loss: 0.022667\n",
      "Epoch 972, Train Loss: 0.022964, Val Loss: 0.022363\n",
      "Epoch 973, Train Loss: 0.022811, Val Loss: 0.022441\n",
      "Epoch 974, Train Loss: 0.022837, Val Loss: 0.022541\n",
      "Epoch 975, Train Loss: 0.022818, Val Loss: 0.022627\n",
      "Epoch 976, Train Loss: 0.022855, Val Loss: 0.022482\n",
      "Epoch 977, Train Loss: 0.022780, Val Loss: 0.022404\n",
      "Epoch 978, Train Loss: 0.022843, Val Loss: 0.022483\n",
      "Epoch 979, Train Loss: 0.022836, Val Loss: 0.022441\n",
      "Epoch 980, Train Loss: 0.022776, Val Loss: 0.022416\n",
      "Epoch 981, Train Loss: 0.022803, Val Loss: 0.022371\n",
      "Epoch 982, Train Loss: 0.022766, Val Loss: 0.022553\n",
      "Epoch 983, Train Loss: 0.022817, Val Loss: 0.022407\n",
      "Epoch 984, Train Loss: 0.022832, Val Loss: 0.022507\n",
      "Epoch 985, Train Loss: 0.022774, Val Loss: 0.022591\n",
      "Epoch 986, Train Loss: 0.022826, Val Loss: 0.022742\n",
      "Epoch 987, Train Loss: 0.022711, Val Loss: 0.022398\n",
      "Epoch 988, Train Loss: 0.022702, Val Loss: 0.022914\n",
      "Epoch 989, Train Loss: 0.022790, Val Loss: 0.022346\n",
      "Epoch 990, Train Loss: 0.022656, Val Loss: 0.022385\n",
      "Epoch 991, Train Loss: 0.022803, Val Loss: 0.022319\n",
      "Epoch 992, Train Loss: 0.022740, Val Loss: 0.022406\n",
      "Epoch 993, Train Loss: 0.022739, Val Loss: 0.022526\n",
      "Epoch 994, Train Loss: 0.022706, Val Loss: 0.022412\n",
      "Epoch 995, Train Loss: 0.022777, Val Loss: 0.022929\n",
      "Epoch 996, Train Loss: 0.022789, Val Loss: 0.022692\n",
      "Epoch 997, Train Loss: 0.022761, Val Loss: 0.022236\n",
      "Epoch 998, Train Loss: 0.022693, Val Loss: 0.022383\n",
      "Epoch 999, Train Loss: 0.022690, Val Loss: 0.022741\n",
      "Epoch 1000, Train Loss: 0.022804, Val Loss: 0.022361\n",
      "Epoch 1001, Train Loss: 0.022638, Val Loss: 0.022481\n",
      "Epoch 1002, Train Loss: 0.022752, Val Loss: 0.022219\n",
      "Epoch 1003, Train Loss: 0.022670, Val Loss: 0.022695\n",
      "Epoch 1004, Train Loss: 0.022745, Val Loss: 0.022512\n",
      "Epoch 1005, Train Loss: 0.022650, Val Loss: 0.022621\n",
      "Epoch 1006, Train Loss: 0.022722, Val Loss: 0.022287\n",
      "Epoch 1007, Train Loss: 0.022820, Val Loss: 0.022303\n",
      "Epoch 1008, Train Loss: 0.022760, Val Loss: 0.022157\n",
      "Epoch 1009, Train Loss: 0.022640, Val Loss: 0.022363\n",
      "Epoch 1010, Train Loss: 0.022617, Val Loss: 0.022229\n",
      "Epoch 1011, Train Loss: 0.022626, Val Loss: 0.022148\n",
      "Epoch 1012, Train Loss: 0.022668, Val Loss: 0.022555\n",
      "Epoch 1013, Train Loss: 0.022655, Val Loss: 0.022165\n",
      "Epoch 1014, Train Loss: 0.022604, Val Loss: 0.022178\n",
      "Epoch 1015, Train Loss: 0.022620, Val Loss: 0.022366\n",
      "Epoch 1016, Train Loss: 0.022604, Val Loss: 0.022300\n",
      "Epoch 1017, Train Loss: 0.022615, Val Loss: 0.022201\n",
      "Epoch 1018, Train Loss: 0.022661, Val Loss: 0.022196\n",
      "Epoch 1019, Train Loss: 0.022549, Val Loss: 0.022300\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Plots ===\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss over Epochs (REN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Prediction vs Ground Truth ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Y_pred_val = model(X_val).numpy()\n",
    "    Y_true_val = Y_val.numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(p):\n",
    "    plt.subplot(p, 1, i + 1)\n",
    "    plt.plot(Y_true_val[200:600, i], label='True')\n",
    "    plt.plot(Y_pred_val[200:600, i], '--', label='REN Prediction')\n",
    "    plt.ylabel(f'Output {i + 1}')\n",
    "    if i == 0:\n",
    "        plt.title(\"REN Fit on Validation Set\")\n",
    "    if i == p - 1:\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "3bfd33396c789273",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
